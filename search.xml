<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>笔记本散热不完全指南</title>
    <url>/posts/23143/</url>
    <content><![CDATA[<h2 id="散热调校思路"><a href="#散热调校思路" class="headerlink" title="散热调校思路"></a>散热调校思路</h2><p>笔记本电脑的内部空间寸土寸金，所以一般情况下不需要对散热模组做任何修改，在开始动手前，我们需要明确几个事情：</p>
<ol>
<li>笔记本电脑，尤其是高性能笔记本电脑在高性能输出模式下，一定会非常烫，CPU温度在90度上下是正常情况。</li>
<li>CPU和GPU的性能在理想情况下都可以100%输出，但理想情况是少数，所以如果是为了更好地运行游戏，优先保障GPU性能释放。</li>
<li>笔记本电脑需要定期进行维护，且维护周期小于台式机电脑，灰尘多的环境推荐6个月，其他环境则为12个月。</li>
<li>本指南是为了在尽可能不改动原有硬件设计的前提下，最大程度释放电脑的性能。<span id="more"></span></li>
</ol>
<h2 id="Step-0-准备工作"><a href="#Step-0-准备工作" class="headerlink" title="Step 0  准备工作"></a>Step 0  准备工作</h2><p>⚠<strong>风险提示</strong><br>本文包含的所有测试、性能参数调整均有一定几率损坏您的笔记本电脑，笔者无法保证没有任何意外情况的发生。请在实际动手之前，确认您已做好了以下心理建设工作：</p>
<ul>
<li>不怕弄坏电脑的钱包厚度</li>
<li>不摔杯子的耐心</li>
<li>不糊弄的态度</li>
</ul>
<p>每一台电脑的使用环境，性能输出设定都不尽相同，在解决问题之前我们需要先定位问题，所以你可能需要以下几款性能监测软件：</p>
<ol>
<li>HWMonitor</li>
<li>Cinebench R23</li>
<li>FurMark<br><strong>建议避免使用鲁大师</strong></li>
</ol>
<h3 id="CPU散热测试"><a href="#CPU散热测试" class="headerlink" title="CPU散热测试"></a>CPU散热测试</h3><p>打开Cinebench R23与HWMonitor，切换到Cinebench软件窗口，点击Multi后面的Start，此时观察HWMonitor软件中的：</p>
<ul>
<li>CPU Clock</li>
<li>CPU Power</li>
<li>CPU Temperature<br>三项性能读数。</li>
</ul>
<h3 id="CPU测试结果分析"><a href="#CPU测试结果分析" class="headerlink" title="CPU测试结果分析"></a>CPU测试结果分析</h3><ol>
<li>CPU温度从来没有达到过95℃以及更高，CPU核心频率没有出现下降，CPU功率没有降低，那说明此时笔记本电脑的散热系统状态尚可，需要进一步测试GPU。</li>
<li>CPU温度在测试开始的60秒内迅速爬升至100℃，几分钟后核心频率与功耗大幅度降低，说明该电脑需要进行全面彻底的清洁维护工作，见后续内容，请跳过后续散热测试环节。</li>
<li>CPU温度缓慢爬升至100℃，几分钟后下降至80 - 85℃，同时核心频率与功耗降低，但降低幅度不大，说明此时散热系统需要一定的清洁与维护，已经无法压制住默认设定下的CPU性能输出，可以考虑直接进行清洁维护工作。</li>
</ol>
<h3 id="双烤散热测试"><a href="#双烤散热测试" class="headerlink" title="双烤散热测试"></a>双烤散热测试</h3><p>当Cinebench R23的测试运行完毕后，静置电脑5 - 10分钟，等CPU恢复到开始测试之前的温度，然后打开Furmark软件，不用做任何设置，直接点击开始即可将GPU拉到100%负载，接着继续运行Cinebench R23的压力测试，此时您的笔记本已经处于“双烤”模式下，即CPU与GPU都满载运行。</p>
<p><strong>⚠如果您的笔记本电脑长时间没有清理与更换硅脂，请谨慎进行此项测试！！</strong></p>
<p>继续观察HWMonitor软件里面的读数，此时需要多观察3项指标：</p>
<ol>
<li>GPU Clock</li>
<li>GPU Power</li>
<li>GPU Temperature<br><strong>至少运行20分钟以确定散热系统的极限</strong></li>
</ol>
<h3 id="双烤散热结果分析"><a href="#双烤散热结果分析" class="headerlink" title="双烤散热结果分析"></a>双烤散热结果分析</h3><p>99.9%的笔记本电脑CPU在双烤15分钟内会出现一定程度的核心频率与功率的降低，此时就可以判断出一款游戏笔记本当前状态下的散热水平：</p>
<ul>
<li>GPU核心频率与功耗没有任何变化，说明此游戏笔记本电脑的散热设计出色，一切正常，可以不用进行任何维护与性能调校，继续用就OK。</li>
<li>GPU核心与功率有部分下降，但幅度不大，保持在20%以内，在经过本文后续部分的调校后可以保持GPU全程100%性能输出。</li>
<li>CPU和GPU核心频率与功耗同时大幅下降，幅度超过50%，则需要对散热模组进行一定的改造才能保持GPU性能的输出（很不幸，笔者的笔记本电脑属于这一类）。</li>
</ul>
<h2 id="Step-1-清灰与更换硅脂的注意事项"><a href="#Step-1-清灰与更换硅脂的注意事项" class="headerlink" title="Step 1 清灰与更换硅脂的注意事项"></a>Step 1 清灰与更换硅脂的注意事项</h2><p><strong>请您自行在B站、百度贴吧等平台搜索对应型号笔记本电脑的拆机教程。</strong><br>拆机清灰与更换硅脂前，请准备好：</p>
<ol>
<li>型号齐全，质量可靠的螺丝刀具，笔记本电脑的螺丝都非常小，且质量良莠不齐，如果滑丝，虽然对正常使用一般没有影响，但心里会有疙瘩。</li>
<li>一个防静电手环或脚环，注意接地，如果不想麻烦，可以在动手前摸一摸铁制楼梯扶手。</li>
<li>带磁性的金属盘用来存放拆解完毕的螺丝。</li>
<li>用于更换的硅脂，笔者推荐使用：信越7868，或者其他手边上有的硅脂。不推荐使用液态金属。<br>近几年上市的游戏笔记本电脑的维护工作相当便利，除了某些将主板反装的型号外。只要将底面螺丝全部拧开即可卸下面板。</li>
</ol>
<p>⚠<strong>请拔掉电池排线，让电脑彻底断电</strong>。</p>
<p>将散热模组拆卸后，需要将主板上CPU与GPU核心上的残余硅脂清理干净，散热模组上的硅脂也同样需要清理，使用普通的湿纸巾就有比较好的清洁效果，有没有酒精都无所谓。如果距离上一次清理时间很久（超过24个月），或者干脆买来就没有清理过，建议将散热风扇与热管的连接处拆开，一般会用胶带粘上，撕开，然后你会发现惊喜。笔者的Dell G7 7590，因为家里有猫，所以某一次拆开连接处后发现了一层紧密的猫毛+灰尘混合物，整体散热效果可想而知。</p>
<p>移动平台的CPU与GPU核心部分面积相对较小，所以更换硅脂的时候挤黄豆粒大小就可以，可以手动涂抹均匀，也可以直接将散热模组压上去，散热效果没有区别。</p>
<p>⚠<strong>请把电池排线插回去再安装底部面板。</strong></p>
<h2 id="Step-2-检验清灰与更换硅脂效果"><a href="#Step-2-检验清灰与更换硅脂效果" class="headerlink" title="Step 2 检验清灰与更换硅脂效果"></a>Step 2 检验清灰与更换硅脂效果</h2><p>当你完成了前面的步骤后，再次参照Step 0进行一次双烤测试，这一次至少运行30分钟，可以让刚刚更换的硅脂充分与芯片接触，直到CPU与GPU的核心频率与功耗不再变化：</p>
<ol>
<li>如果GPU核心频率保持双烤测试开始时的读数，不再降低，那么您的笔记本电脑可以正常使用了，如果您想进一步挖掘CPU的性能潜力，可以继续阅读后续内容。</li>
<li>如果GPU频率还是随着时间流逝而降低，请将稳定后的CPU与GPU频率、功率读数记录下来作为我们后续性能调校的基准。</li>
</ol>
<h2 id="Step-3-软件调校"><a href="#Step-3-软件调校" class="headerlink" title="Step 3 软件调校"></a>Step 3 软件调校</h2><h4 id="原理解析"><a href="#原理解析" class="headerlink" title="原理解析"></a>原理解析</h4><p>在实际动手之前，我们先了解一下调校原理：</p>
<p>P &#x3D; U * I</p>
<p>这是初中物理就介绍过的功率计算公式，P代表功率，U代表电压，I代表电流。在前文反复提到的保障GPU性能输出，实际上就是尽可能让GPU功率保持在标定的最大值，比如笔者电脑的GPU设计最大功率为80W，只要能够让GPU一直以80W运行，就可以输出100%的性能。但是如果CPU和GPU的温度太高，超过了阈值温度一段时间，主板内部的保护程序就会介入，强行降低两者的功率达到降温的效果，避免硬件损坏。</p>
<p>这也就是我们用笔记本电脑玩游戏刚开始不会卡，玩到一半就掉帧的原因，刚开机的时候，散热系统并没有过载，所以主板尽可能让CPU与GPU以最大功率运行，等温度超过阈值后就强行降低，造成严重的卡顿和掉帧。</p>
<p>解决的思路也很简单，因为笔记本内部空间实在太小，CPU的运行温度通常都会高于GPU，所以我们直接提前降低CPU的最大功率，确保不会因为CPU温度过高而连带着GPU的温度超过阈值。只要主板的保护程序不介入，我们的GPU就可以一直以最大功率运行，确保游戏不掉帧。</p>
<h4 id="ThrottleStop"><a href="#ThrottleStop" class="headerlink" title="ThrottleStop"></a>ThrottleStop</h4><p>我们以目前市场保有量最大的Intel CPU为例，本指南并不使用Intel官方的Intel XTU软件来进行调校，因为Intel XTU的使用上略显繁琐，而且十分臃肿。我们使用一款第三方软件ThorttleStop来完成Intel CPU的性能调校，AMD Ryzen移动处理器也有类似的软件，叫做Ryzen Contoller，原理一样。</p>
<p>ThrottleStop有两种方式来降低CPU的温度：</p>
<ol>
<li>直接调节CPU的功率上限</li>
<li>精确控制CPU核心频率与电压<br>我们从功率上限的调整开始，打开ThrottleStop后，点击TPL按钮，直接修改PL1与PL2的值，保存后即可生效，但我们怎么确定应该改成多少呢？</li>
</ol>
<p><img src="/images/hd_01_01.png" alt="ThrottleStop"></p>
<p>还记得之前记录的双烤后稳定的CPU与GPU功率吗，将这两个数字相加就得到了您笔记本散热系统目前可以压制的功率上限，比如CPU稳定在35W，GPU稳定在105W，那么此时散热系统最多可以冷却140W的热功率。假设GPU标定最大功率为120W，那么用140 - 120，就得到了极限状态下GPU不降频时，CPU的最大功耗，即20W。</p>
<p><strong>温馨提示：您可以在商品宣传页、B站、百度贴吧或者沙雕群友处获取您笔记本电脑所标定的CPU与GPU功率。</strong></p>
<p>不过需要注意的是，双烤是非常极端的情况，一般玩游戏的时候CPU并不会100%满载，只有GPU会，所以可以适当放宽CPU的功率上限，比如相减得到20W，可以设定为25W，留有一定的冗余让CPU的性能多释放一点。</p>
<p>笔者更推荐第二种精确控制CPU核心频率与电压的方式，因为不仅可以降低CPU的温度，甚至还可以提高CPU的性能输出。降低温度的部分就不提了，说一下如何提高CPU的性能。<br>还是请出上面那个公式：</p>
<p>P &#x3D; U * I</p>
<p>CPU的性能输出很大程度上受到核心频率的影响，核心频率由电流来控制，我们可以在功率P上限不变的情况下，通过降低电压 U 来提高CPU的核心电流 I，达到提高核心频率的效果。</p>
<p>以笔者的Dell G7 7590所得到的测试结果来验证一下这个思路。<br>测试条件：</p>
<ul>
<li>室温：25℃</li>
<li>CPU型号： Intel i7 8750H， TDP：45W</li>
<li>机身未垫高，无其他辅助散热措施</li>
</ul>
<p><img src="/images/hd_01_02.png" alt="测试结果"></p>
<p><img src="/images/hd_01_03.png" alt="测试结果2"></p>
<p>将所有调校项目还原，原厂状态下运行Cinebench R23的多核测试项目，连续运行5次取平均值，从表中的结果可以看到，原厂状态下的最高温度超过90℃，而R23多核得分为最低的5033分，因为在功率稳定在45W时，CPU的核心频率仅为2853MHz。<br>其它3组测试中，笔者将核心电压降低0.154V，同样运行5次测试，由于电压降低，所以当CPU功率稳定在45W时，电流增加，核心频率相比原厂状态有了400Mhz的提升，得到了超过16%（1030分）的性能提升。<br>更加有意思的是，当笔者将最大频率从3893Mhz降低到3500MHz后，最后的性能几乎没有损失，这是因为最大频率需要在超过标称功率的情况下才能达到，而这种状态只能维持很短时间，最后在45W时，频率都会降低到3250Mhz，所以性能没有变化。<br>最后一组测试将最大频率直接拉低到3000MHz，此时CPU的稳定功率从45W降低到了36W，但由于电压更低，实际上核心频率反而比原厂状态更高，所以性能输出也更强，且最大温度直接降低14度，稳定输出时的温度也降低了6度。</p>
<p>表中0.154V这个数据是笔者经过多次反复尝试，验证后得到的最佳参数，如果您也想尝试给CPU降低电压，建议从0.05V开始，并且以0.025V为一个档位不断累加，每次调整后建议用AIDA64运行压力测试至少5分钟，如果没有出现蓝屏、死机的情况就继续递进，直到出现蓝屏或者您满意了为止。后续日常使用遇到反复蓝屏、死机，适当调高一点电压即可解决问题。<br>实际上ThrottleStop里面不仅可以调整CPU的核心电压，还能修改内置显卡iGPU，核心缓存以及其它unCore部分电压，笔者除了核心电压是0.154V以外，其它全部统一降低了0.1V，目前稳定运行超过一年，没有任何问题出现。</p>
<p><img src="/images/hd_01_04.png" alt="选项说明"></p>
<h3 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h3><ul>
<li>想要进一步压榨CPU的持续输出能力，降低电压，不需要修改功率上限</li>
<li>想要大幅度降低CPU温度，降低电压的同时也降低核心频率上限</li>
</ul>
<p>ThrottleStop可以存储多个调校方案，笔者设置了3个方案：</p>
<ol>
<li>3A游戏，将核心频率锁定在3.0GHz，确保GPU永远不会降低功率造成卡顿</li>
<li>轻度游戏，将核心频率锁定在3.5GHz，尽可能让CPU多干点活儿</li>
<li>日常模式，仅降低电压，不修改核心频率上限，让CPU火力全开<br>配合自定义的快捷键，可以在一秒钟内无缝切换多个性能模式，不需要重启，在游戏中也可以瞬时生效，非常方便快捷。</li>
</ol>
<h3 id="吐槽"><a href="#吐槽" class="headerlink" title="吐槽"></a>吐槽</h3><p>首先我要感谢一下Dell 游匣系列的硬件工程师与软件工程师，我于2019年的1月份购入这台Dell G7 7590，时值寒冬，所以当时散热并没有什么问题，性能输出一切正常。可是到了夏天，我的噩梦就开始了，当时看中这款笔记本是因为比较内敛，没有主流游戏笔记本张扬的造型，相对来说也比较轻薄。可是轻薄的机身并不能搭载足够强力的散热系统来压制14nm+++++++++的i7 8750H，夏天开了空调的情况下也无法有效压制CPU的温度，导致了BIOS强行介入，按照正常厂商的逻辑，当温度太高的时候，游戏笔记本需要优先考虑的肯定是GPU的性能输出。可是聪明的Dell软件工程师们决定确保CPU的性能输出，GPU的核心频率与功率直接砍到5分之1，而CPU核心还在不停地往上窜，所以这篇指南能够和大家见面需要感谢一下Dell团队里面的卧龙凤雏。</p>
<p>希望这篇指南能够或多或少的帮助到您，也希望您的笔记本电脑能够在这一通操作后顺利开机，我们下次再见！</p>
]]></content>
      <categories>
        <category>hardware</category>
      </categories>
      <tags>
        <tag>tutorial</tag>
        <tag>gcores</tag>
        <tag>laptop</tag>
        <tag>cooling</tag>
      </tags>
  </entry>
  <entry>
    <title>低成本NAS解决方案1.0</title>
    <url>/posts/22694/</url>
    <content><![CDATA[<p>作为一个十多年前接触ACG的大龄青年，最常用也是最喜爱的各种影视剧观赏方式还是非常原始的找资源，下载然后本地播放的路径。毕竟当时的在线视频播放网站要么是画质奇低，要么就是资源门类不全，要看美剧得找这个网站，看动画是另外一个。我个人又喜欢同时开坑很多部作品，经过高中三年和大学的积累，硬盘里面保存的各路影视剧集已经超过了人脑能够记忆每一部观看进度的数量，所以我开始研究一套方案来解决这个问题。</p>
<span id="more"></span>

<h2 id="出租屋1-0"><a href="#出租屋1-0" class="headerlink" title="出租屋1.0"></a>出租屋1.0</h2><p>标题写了低成本，所以我也贯彻花小钱办大事的原则，开始在网络上检索相关的软硬件的搭配思路。首先想到的肯定是以前用过的Kodi，作为一款历史悠久的媒体管理软件，Kodi多平台支持和完善的插件系统让我首先选择了它，最最最重要的是Kodi完全免费，低成本嘛！</p>
<p>第一步是在我当时的小米电视3S上安装了Kodi客户端，然后用移动硬盘盒把闲置的2.5寸机械硬盘插到了路由器的USB口上，R6300V2的USB口是USB 3.0的速度，应付2.5寸机械硬盘绰绰有余了。接着就是设置路由器，打开SMB共享，然后在小米电视上的Kodi内添加SMB路径，让Kodi自己识别硬盘内的媒体资源。</p>
<p><img src="/images/nas_01_01.png" alt="1.0"></p>
<p>1.0版本的优缺点如下：</p>
<p>Pros：</p>
<ul>
<li>0成本，所有的设备都是现成的</li>
<li>简单快捷，就装了一个Kodi</li>
</ul>
<p>Cons：</p>
<ul>
<li>没有实现多设备观看进度同步</li>
<li>Kodi界面有点丑</li>
<li>刮削效果十分一般</li>
</ul>
<p>因为时间确实太久了，手边上没有留图片，Kodi默认主题的UI布局是很一般的，而且这个方案没有解决我最大的痛点，因为我经常用电视看到一半就躺床上接着看了，这时候就要我自己记住看到了几分几秒，然后用手机去拖到对应的地方，小米电视上的Kodi倒是能记住在电视机上看到了哪里，但不能同步到手机。纯Kodi环境也能做到这一点，但是巨麻烦，要把Kodi内的某个文件共享，保存到一个第三方服务器上来同步进度，所以我开始了2.0版本的折腾。</p>
<h2 id="出租屋2-0"><a href="#出租屋2-0" class="headerlink" title="出租屋2.0"></a>出租屋2.0</h2><p>在被小米电视折磨了很久之后，终于下定决心换电视了，当时也没考虑别的，直接买了Sony 55X9000F，因为只买得起这个。主力观影设备从1080p升级到4K HDR之后，我也有动力去更新媒体播放的部分，主要思路基本不变：</p>
<ul>
<li>存储：移动硬盘</li>
<li>访问：路由器SMB共享</li>
<li>播放：电视里的Kodi客户端</li>
</ul>
<p>在这里我增加了一个设备和软件来实现我一直需要的多设备观看进度同步的功能，大名鼎鼎，无所不能的斐讯N1。所以整体架构变成了如下：</p>
<p><img src="/images/nas_01_02.png" alt="2.0"></p>
<p>我顺手把路由器换成了一直用到现在的ASUS AC86U，宽带也换成了联通的100M。引入了斐讯N1之后，我最大的痛点就被解决了，Emby作为一个曾经开源的媒体管理软件，基础的功能做的还是很不错的，不需要任何设置就可以实现网络内的观看进度同步功能，还可以自己下载海报、完善媒体信息，前提是媒体文件命名是按照相应的规则，不然也会出现识别不出来或者错误的情况。</p>
<p>2.0版本的优缺点如下：</p>
<p>Pros：</p>
<ul>
<li>解决了最大的痛点，多设备观看进度同步</li>
<li>Emby的Kodi皮肤美观，功能强大</li>
</ul>
<p>Cons：</p>
<ul>
<li>Emby服务器跑在N1上，响应速度十分缓慢</li>
<li>Sony电视芯片组解码能力弱鸡，无法应对高码率的4K BDRip片源</li>
</ul>
<p>2.0的版本更像是一个设计思路的验证，这套配置我大概凑合用了一年左右，因为N1和Sony电视搭载的arm芯片都谈不上性能强劲，加上Emby开发组主要精力都放在x86平台上，对arm的优化不是特别好，所以Emby Server on Android的处理效率十分低下，我当时都是在社区里面下载的，他们根本没有放在官网上，你就懂了。比如我看完一集动画，按照常理，回到主菜单应该会马上更新下一集让我选择，但是当时会停在那里，等一分钟后才会在主界面更新下一集的提示内容，而且观看进度的同步也需要很长的响应时间，但，又不是不能用。</p>
<p>Emby和Kodi怎么组合起来使用在我这一篇投稿里面有详细说明，网上也有很多详细的教程可以参考。</p>
<p>这一阶段的投入为80元，花在了海鲜市场淘的斐讯N1，不换电视机和路由器不影响这一套系统的搭建。</p>
<h2 id="出租屋3-0"><a href="#出租屋3-0" class="headerlink" title="出租屋3.0"></a>出租屋3.0</h2><p>因为电视升级到了4K，所以下载的片源也相应的需要提高到4K的标准，3T的移动硬盘有点扛不住了，经常需要删东西腾地方，让我下定决心砍手的还是在张大妈看到了星际蜗牛的垃圾可以捡，于是在海鲜市场淘了一个星际蜗牛C款的空机箱，准备自己装一台NAS。</p>
<p>具体的配置如下：</p>
<ul>
<li>机箱：星际蜗牛C款    ¥ 128</li>
<li>CPU：Asrock Intel J3455     ¥  499</li>
<li>内存：闲置笔记本DDR3 4GB x 2   ¥  0</li>
<li>电源：益衡Flex 250W ¥ 243</li>
<li>风扇：闲置海盗船12CM风扇     ¥ 0</li>
<li>启动盘：闲置16GB U盘   ¥ 0</li>
<li>网卡：Intel 82576 千兆双口  ¥ 89</li>
<li>不含硬盘合计：¥ 959</li>
</ul>
<p>因为我准备长期使用这台NAS，所以在电源上就没有省钱，直接买了80 Plus铜牌认证的电源，并且把星际蜗牛机箱自带的SATA背板拆了，直接通过SATA线连接硬盘，确保不会因为供电问题导致硬盘损坏。整体空箱成本控制在了1000附近，就算自己去买内存条也不会特别贵，而且不需要上8GB内存，有4GB就绰绰有余了，但是NAS没有硬盘怎么能行呢，所以我又开始了找硬盘，第一件事情就是确定自己需要多大的存储空间。然后我就分析了一下自己的需求：</p>
<ol>
<li>媒体资源存储</li>
<li>重要文档备份</li>
<li>照片备份</li>
<li>私人云盘</li>
</ol>
<p>其中2、3、4项需求是重合的，而且所需要的空间不会特别大，然后看了看自己的钱包，我把总体存储空间的指标设置在了12 - 16T，而且不打算使用任何形式的Raid。首选肯定是各家专门的NAS产品线，比如希捷的酷狼和西数的红盘，不过在转了一圈之后发现红盘出问题的概率较高，直接pass，也有考虑过用希捷酷鱼，毕竟很便宜，我要保存的也不是特别重要的数据，结果在马云C店找到了工包的酷狼，价格便宜了40%左右，最后拿下了4块4T的酷狼，合计才2160块钱，当时京东盒装价格在900左右。果断下单，到手后随便看了一下触点和通电时间就插上去装黑群晖了。</p>
<p>最后NAS的总共花费是¥ 3119，还不如同配置的Synology DS 918+贵。</p>
<p>安装过程不表，网上的教程不说到处都是，也可以说是随处可见了，选择的是当时J3455最稳定的DSM 6.17版本，事实证明确实是超级稳定，既然有了专门的NAS，自然要把之前2.0版本的问题给解决掉。首先是装了Docker，在Docker里面把本来跑在N1的Emby和路由器上的qbittorrent给迁移了过来。然后为了解决Sony电视解码能力弱的问题，我又给斐讯N1刷入了Corelec 9.2的固件。Corelec是一个专门针对电视盒子的固件，可以大幅度提高解码能力，我自己测试的情况是，使用斐讯 N1的全功能固件只能用Kodi解码4K 50Mbps左右码率的片源，再高就会掉帧，刷了Corelec 9.2之后，80 - 90Mbps的片源都没有问题。</p>
<p><img src="/images/nas_01_03.png" alt="3.0"></p>
<p>这就是我目前使用的这套方案，每一个环节需要的时间和精力都不多，而且有非常多的教程、文档可以参考，我没有选择装unraid也是这个原因。到目前已经使用了接近2年时间，所有4块工包硬盘没有任何问题，但是我不推荐你们去买这种硬盘，毕竟不是所有人都能承担这种风险，目前稳定通电了15916小时（写这篇文章之时）。</p>
<p><img src="/images/nas_01_04.png" alt="HDD"></p>
<p>我应该不会继续再升级、折腾这一套系统了，响应速度、解码能力和电视播放效果都很不错了，考虑到总共只花了这么多钱，还是挺可以的。</p>
<p>接下来聊一下为什么我选择了不使用任何形式的Raid，这台NAS里面保存的大部分数据都是非敏感性、可重复获取的媒体资源，硬盘坏了也不心疼，我真正需要备份留存的数据很少，所以我才用的是本地备份+云端同步的方案，通过Hyper Backup软件，每天自动对选定的文件夹备份到另外一块硬盘上，然后还链接到我的OneDrive自动同步，有了双重保险，对我来说这个方案实现了安全和可用容量的均衡，如果用raid 5或者unraid方案，对我来说直接损失了25%的可用容量，这个是我不能接受的，毕竟只有4个盘位，再大的我也用不上。</p>
<p>3.0版本的优缺点：</p>
<p>Pros：</p>
<ul>
<li>性能可靠、强大，Emby响应速度，电视解码能力都很不错</li>
<li>使用省心，稳定，只要我自己不手痒，一年不重启都没问题</li>
<li>可拓展性强，Docker让一切都有可能</li>
</ul>
<p>Cons：</p>
<ul>
<li>风扇有点吵</li>
<li>长沙联通还我公网IP</li>
</ul>
<p>最后放一下完整的拓扑图和各个平台的效果图吧。</p>
<p><img src="/images/nas_01_05.png" alt="604拓扑图"></p>
]]></content>
      <categories>
        <category>nas</category>
      </categories>
      <tags>
        <tag>nas</tag>
        <tag>kodi</tag>
        <tag>synology</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>新纪元主机的暗线斗争</title>
    <url>/posts/43077/</url>
    <content><![CDATA[<h2 id="1"><a href="#1" class="headerlink" title="1"></a>1</h2><p>时间是公元1998年，在微软的某个实验室中，一小撮工程师分解了几台Dell笔记本，将其组装成了一台基于Windows系统的游戏主机，这个小组希望创造一台可以和Sony PlayStation 2媲美的主机。</p>
<p>微软已经通过Windows 95以及一系列软件稳稳的占据了PC软件行业的领先地位，此时的比尔·盖茨正谋求着人们办公桌以外更大的版图——客厅。Sony的PlayStation主机正在主机市场上大杀四方，不久后PlayStation 2也即将发布，越来越多的游戏开发者开始为非Windows平台开发游戏，这对微软来说也不是一个好消息。微软决定全面进入家用游戏主机市场，在Xbox研发的过程中，一家图形公司进入了他们的眼帘，这就是Nvidia。</p>
<span id="more"></span>

<p><img src="/images/console/bill_gates_xbox.jpg" alt="bill gates xbox"></p>
<p><img src="/images/console/titanxputinforweb1.png" alt="titanxputinforweb1"></p>
<p>在上个世纪末的游戏业界，主机游戏的开发与PC游戏的开发远没有现在这么统一，追根溯源是因为两者之间的硬件基础可谓天差地别。不同品牌的主机，对游戏画面、游戏音效的实现原理完全不同，这就意味着游戏的开发流程也必须是独一无二的。主机游戏画面的3D化普及，是通过1993年发售的初代PlayStation才得以实现，而碰巧的是3D画面的PC游戏，自1992年发售的《德军总部：3D》后才开始井喷。两者从2D向3D的转换基本是同一时间节点，但两者的实现方式却截然不同。简而言之，PlayStation通过单独定制的特殊芯片来处理3D模型，而《德军总部：3D》是通过一种特殊的算法来表现画面，欺骗人类的大脑，从而营造出3D的感觉。</p>
<p>​<img src="/images/console/ps1Ridge_Racer.jpg" alt="ps1Ridge Racer"></p>
<p><img src="/images/console/wolfenstein_3d.jpg" alt="wolfenstein 3d"></p>
<p>这种情况一直持续到了微软与Nvidia之间命运的合作。在Xbox研发过程中，微软希望打造一款全能的游戏主机，可以玩游戏，上网，运行的是Windows操作系统，甚至能通过互联网进行联机游戏。而且微软为了追求极致的画面效果，没有和传统的主机厂商一样，完全自行研发Xbox硬件平台，而是找到了当时已经开始崭露头角，显现出王霸之气的Nvidia。</p>
<p>Nvidia是一家华人创立的科技公司，在公司设立之初就以高性能的图形加速卡为主打产品。PC发展早期，所有的图像输出都是由CPU来直接控制的。CPU直接在内存里写入数据，然后显示器再根据内存里的数据进行显示。图形加速卡就是为了加速这一过程而诞生的产物，至于现代意义上的显卡，那时候还没有出现。</p>
<p>​				</p>
<p><img src="/images/console/voodoo_1.jpg" alt="voodoo 1"></p>
<p>在1999年，Nvidia发布了全球第一款图形处理芯片——Geforce 256，正式确立了“GPU”这一相对于CPU的名称，也将图形加速卡从CPU的附庸地位，提升到了与CPU并驾齐驱的程度。关于这块GPU的性能，在当时就是开天辟地的存在。希望给游戏机市场一个惊喜的微软自然而然地盯上了这个新玩意儿。双方一拍即合，Nvidia希望抱上微软的大腿，在Windows平台大显身手，而微软需要足够强劲的机能，创造出足够惊艳的游戏体验来吸引已经玩了很多年日本游戏机的玩家们来购买他们的新主机。</p>
<p><img src="/images/console/GeForce_256.jpg" alt="GeForce 256"></p>
<p>这一合作的结晶，就是Xbox。由此游戏主机厂商和GPU厂商在历史的机缘巧合下，正式的走到了一起，而这将给游戏业界带来翻天覆地的变化。</p>
<p>​				</p>
<p><img src="/images/console/xbox.jpg" alt="xbox"></p>
<h2 id="2"><a href="#2" class="headerlink" title="2"></a>2</h2><p>我们来回想一下Xbox的主要特点：</p>
<ul>
<li>大，手柄大，机器大</li>
<li>性能强劲</li>
</ul>
<p>Sony的PlayStation 2首发价格为299美金，而性能差不多是它两倍的Xbox的首发价格，没错，也是299美金。微软以亏本的价格发售Xbox，不仅研发投入耗费巨资，而且整机的硬件成本也高居不下,一时间造成了卖的越多亏的也越多的局面。联想到几年后Sony和微软的立场对调过来，也是令人感叹。</p>
<p>Xbox的发售对游戏业界的影响并不仅仅是多了一个巨大体量的竞争者，还在于它对整个游戏开发生态带来全新的事物，那就是Windows和它的DirectX。</p>
<p>​<img src="/images/console/DirectX.jpg" alt="DirectX"></p>
<p><img src="/images/console/windows_xp.jpg" alt="windows xp"></p>
<p>Windows大家都知道是什么，那这个DirectX又是什么东西呢？这里我尽最大努力讲的简单明了一点。</p>
<p>电脑（游戏主机）的硬件，都是一堆集成电路拼在一起，放在一块板子上，这是物理性的基础。而为了使这些零部件运行起来，我们需要给它通电，提供电源，还需要对它们发出<em>指令</em>，按照我们的意愿来工作，不然它们就只是一堆金属和硅。在这个过程中，这些<em>指令</em>就是开发者所编写的程序（统称）。</p>
<p>在游戏主机中，游戏开发者是可以直接下达指令来命令游戏主机的硬件如何工作。这样做的好处是效率大大提升，但问题在于游戏中所表现的一切都要开发者手把手来实现，工作量巨大。那么有没有可能，把游戏所需要的常用指令统合起来，做成一个合集，让开发者减少重复性的劳动，来专心开发新的内容，实现自己的想法呢？</p>
<p>DirectX就是微软为Windows量身打造的一套API（合集），让Windows平台的游戏软件可以高效率的调用硬件资源，让显卡发挥出更高的效率。</p>
<p>我们来说说Nvidia，和Xbox的销售状况不同，Nvidia的GeForce系列显卡销量是节节攀升，市场占有率进一步扩大，不仅将自己的竞争对手消灭到只有一家，而且在产品的研发与更新上处于绝对领先的位置。先进的技术保障了产品的性能优势，而性能优势更能让消费者买单，而且Nvidia的产品策略极其出色，针对不同的市场定位推出了非常有竞争力的产品，整个Nvidia的形式不是小好，而是一片大好。</p>
<p>Nvidia不仅为Xbox提供了单独定制的GPU芯片，还积极优化自家显卡产品线与微软DirectX的兼容性，进一步巩固了自己在Windows平台游戏性能的领先地位，由此Nvidia成了Windows平台在游戏领域推广的标志。越来越多的游戏厂商为了给Windows平台开发游戏 ，不仅要适配Windows的DirectX，还要适配Nvidia的产品。在21世纪初期，软硬件的兼容性可是不得不考虑的大问题，就算是技术参数相近的显卡，因为某款驱动的不同，一款游戏的帧数可能出现几十帧的差距。</p>
<p>​<img src="/images/console/nvidia_its_ment.png" alt="nvidia its ment"></p>
<p>DirectX，这一微软为了PC的Windows打造的产品，通过Xbox的发售（Xbox运行定制版的Windows操作系统，所以也支持DirectX，当年Xbox的代号就叫DirecXbox），慢慢的渗透到了主机游戏的开发世界（Xbox系）里。</p>
<p>微软和Nvidia可谓是天作之合，但事情远远没有表面上的那么简单，虽然Nvidia赚的是盆满钵满，但别忘了Xbox可是卖一台亏一台。纵然微软已经实质性的垄断了PC操作系统，但它也是一家商业公司，不可能对一个一直亏损的项目无限的投入。</p>
<p>于是微软和Nvidia摩擦就此发生了。</p>
<p>2000年4月，微软预付了2亿美元作为Nvidia为Xbox单独定制GPU芯片的费用，在Xbox发售后，销售状况不如人意。于是2002年，微软询问Nvidia是否可以降价，毕竟半导体行业的成本会因为技术的进步随着时间而下降。然而Nvidia一口回绝，在这一过程中，双方闹得十分不愉快，一度对簿公堂，但是由于种种原因，两家最终于2003年的2月私下和解。</p>
<p>此时的Nvidia虽然回绝了微软的降价要求，但是它还是没有和微软撕破脸皮，而是选择了继续合作。微软毕竟是业界巨无霸，Nvidia不敢也没有必要得罪微软。微软也还需要Nvidia继续在Windows平台上继续开疆扩土，但是自家的Xbox上，是不会再用Nvidia的芯片了。</p>
<h2 id="3"><a href="#3" class="headerlink" title="3"></a>3</h2><p>在电脑DIY的领域中，有一个需要考虑的因素就是整机的散热能力。在同等技术条件下，一块芯片的性能越高，发出的热量就会越大。而如何快速有效地排出电子元器件所产生的热量，对一个封闭的系统来说尤为重要，这一点Xbox 360的初期型显然是做的不够好得。2005年11月发布的Xbox 360为新一代主机竞赛拉开了序幕，作为最早发布的次世代主机，比PlayStation 3提前发售了11个月又20天，微软为了避免在Xbox时代一直被PlayStation 2打压的情况再次出现，这次选择了先下手为强，但微软再一次翻了车。</p>
<p>虽然Xbox作为一台主机并不是特别的成功，但微软的战略目的还是达到了一部分，成功地在主机市场中获得了一定的份额，站稳了脚跟。微软继续沿用Xbox的研发经验，但这一次的合作商从Nvidia变成了它的死对头，也是唯一的主要竞争者——ATi。</p>
<p>​                                                            </p>
<p><img src="/images/console/ATi.png" alt="ATi"></p>
<p>ATi由4名香港人和1名荷兰人与1985年组建（没错，还是有华人血统，红绿大厂都是华人创立的），公司最初以帮IBM等大型电脑制造商生产显示芯片为主业，1987年正式成为显卡（图形加速卡）零售商。当Nvidia凭借GPU横扫整个显卡市场，将众多竞争对手淘汰出局之时，ATi挺了下来，成为了当时主流市场上唯一能与Nvidia相抗衡的公司。</p>
<p>微软在结束了与Nvidia在主机显示芯片的合作后，找到了它的竞争对手ATi，加上IBM的Xenon系列CPU，整个Xbox 360的硬件基础已经搭好。强大的硬件，配合上微软自身的Windows软件平台，让微软再一次重拾了抢占客厅娱乐中心的雄心壮志。随着互联网在全球的普及，微软已经做好了充足的准备，在次世代主机大战中打一场漂亮的翻身仗。</p>
<p>然而微软在主机市场上，就是状况频出。</p>
<p><img src="/images/console/RED_RING_OF_DEATH_by_juutin.jpg" alt="RED_RING_OF_DEATH_by_juutin"></p>
<p>提起Xbox 360，三红是绝对无法绕开的关键词。当年三红发生的概率推测为68%，可实际上基本所有的初代Xbox 360都三红了，除了那些堆在仓库还没有开过机的产品以外。微软从来没有正式公布过三红发生的原因，只是用自己雄厚的资金一台一台地收了回来，给玩家更换新的主机。有许多第三方发表过自己的调查意见，其中主要的说法为：</p>
<blockquote>
<p>此问题出现全系列Xbox 360游戏主机中。当主机运行时，主机内部图形处理器发出大量热量， 而散热风扇的速度已经无法把全部热空气抽离，导致主板暴露在高温膨胀变形，一旦主板发生变形，CPU处理器和GPU图形处理器与主板的焊接处脱离，引起主机工作异常，从而导致三红灯警告的出现。</p>
</blockquote>
<p>概括起来就是显示芯片发出的热量实在太大了，那么这块来自于Nvidia竞争对手的GPU到底是为何如此“火辣”呢？</p>
<p>芯片制造领域有一个非常重要的参数，就是制程。CPU和GPU芯片的制程在21世纪之前的单位是微米(μm)，与公制单位的换算比例如下</p>
<ul>
<li>1 000 000微米 （μm）&#x3D; 1 <a href="https://zh.wikipedia.org/wiki/%E7%B1%B3_(%E5%8D%95%E4%BD%8D)">米</a>（m）</li>
</ul>
<p>到了新世纪之后，人类最顶尖的制造工艺终于从微米迈入了纳米的级别，在Xbox 360初期型号上，CPU与GPU的制程都为90nm。我简单说一下为什么这个制程对处理芯片如此重要。</p>
<p>芯片的运算能力大体上由它包含的半导体元器件数量所决定，那么在一定的尺寸下所能容纳的元器件数量越多，它的性能就越强。但是芯片是不可能无限制堆叠元器件数量的，因为这些元器件在工作的过程中会产生热量。而制程就决定了半导体元器件的最小尺寸。如果在制程相对落后的情况下，要提供同样的性能，因为元器件尺寸太大，那么芯片的尺寸会更大，产生的热量更多。如果制程先进，元器件体积变小，甚至能用更小的体积带来更好的性能以及更低的发热量。</p>
<p><img src="/images/console/transistor.jpg" alt="transistor"></p>
<p>90nm的制程在当时来说并不落后，算是主流水平。Nvidia研发实力和技术积累都比ATi来的要强，加上在市场上占主导地位，也有更多的人力物力来进行研发，但ATi的显卡产品在性能上和Nvidia并没有很大的区别，这是为什么呢？因为ATi显卡的核心温度在不同情况下能比Nvidia的高出10℃！这对于PC来说，还可以通过调整散热布局，加装散热风扇来进行改善，加上电脑的机箱尺寸比Xbox 360大的多，所以核心温度高一些并没有太大的问题。</p>
<p>但是对于开箱即用，工作环境一般在电视柜等通风不顺畅的地方，这几度的区别可能就是三红不三红的关键了。为什么微软在Xbox 360搭载了强力芯片的同时，没有设计散热冗余是一个未解之谜，但这次的事件让微软背负了巨大的压力。玩家纷纷咒骂微软，退换手中的主机。如果当初微软选择继续和Nvidia合作，三红还会不会发生也犹未可知啊。</p>
<h2 id="4"><a href="#4" class="headerlink" title="4"></a>4</h2><p>我们将视角从北美转到日本，2006年11月11日，Sony挟裹着PlayStation 2时代的王者之气发布了它们的最新一代主机，想要趁胜追击，一口气将任天堂与微软彻底踩在脚下。</p>
<p>PlayStation 3的硬件设计十分激进，不仅与IBM（还有东芝）合作开发了Cell系列CPU，还和Nvidia牵手，定制了RSX图形处理芯片，力图打造次世代最强的机能，而他们也做到了，但是付出了惨重的代价。</p>
<p>为了保险，Sony希望借助庞大的PlayStation 2现有的游戏库，为新主机添加了向下兼容功能。做法也十分简单粗暴，直接单独添加一块处理芯片来进行硬件运算，加上性能强大的CPU与GPU，这让首发版的PlayStation 3主板上的布局是这样的：</p>
<p>​<img src="/images/console/xbox_360MB.jpg" alt="xbox 360MB"></p>
<p><img src="/images/console/PS3_MB.jpg" alt="PS3 MB"></p>
<p>这样设计的后果便是，整台主机的满载功耗直接飙升到了惊人的380W，Xbox 360的功耗仅为200W左右。这还不是全部，虽然PlayStation 3的机身空间比Xbox 360的厚，但是这并没有带来什么不同，和三红一样的黄灯噩梦出现了。在性能上来说，PlayStation 3上搭载的Nvidia为Sony定制的RSX芯片是要强于Xbox 360上那块ATi芯片的，这也让顽皮狗的《The Last of Us》让人无比惊艳。</p>
<p>Sony也不是彻底迷失在了PlayStation 2的胜利中，它分析并研究了竞争对手的产品，从中学习了它们的优点，比如DirectX。</p>
<p>DirectX是微软给Windows打造的，按主机圈的说法，这叫独占，跨平台是不存在的。但市场上并不是只有微软这一家公司在做图形API，Sony这次找到的就是DirectX的竞争对手——OpenGL。</p>
<p>​				</p>
<p><img src="/images/console/OpenGL.png" alt="OpenGL"></p>
<p>OpenGL正如其名，是开放的，开源的，不如微软那样，只要符合它们的要求和授权，所有人都可以免费使用，甚至是基于它自己做一套相似的东西。而Sony就为PlayStation 3定制了一套PSGL，方便游戏开发者为新的主机更好更快地开发更多的游戏，就如同微软的DirectX在Windows以及Xbox上那样。</p>
<p>但是并没有什么用，问题不在PSGL上，因为它是针对显卡GPU的，问题在Cell上。</p>
<p>模拟地球是很多玩家们调侃PlayStation 3的一句常见台词，指的是当年Sony在宣传的时候打出了新主机能够模拟地球的称号，事实上Cell处理器的性能着实不错，有很多大学、实验室甚至军方都采购了很多的PS3来当作高性能计算机的计算节点。但是好刀也得会用才能上阵杀敌，不然很容易伤到自己。Cell处理器诡异的设计，加上和RSX显示芯片的调度让游戏开发者无比头疼，除了少数几家Sony旗下的工作室的游戏能充分发挥Cell处理器以及RSX的性能外，大部分跨平台游戏的表现都一定程度上比不上Xbox 360版。</p>
<p>虽然Nvidia在这次合作中表现的中规中矩，但这时候的Nvidia已经在PC游戏领域处于压倒性的优势地位中了，高端市场被它牢牢把握，已经成为了事实的GPU市场的领头羊，而主机市场这点利润和繁琐让它有点不胜其烦，特别是随着时间的推进不仅不能涨价，还要不断的调低供货价格，让它更加兴趣索然，这让Nvidia直接离开了主机市场，专攻PC与专业级市场。</p>
<h2 id="5"><a href="#5" class="headerlink" title="5"></a>5</h2><p>Advanced Micro Devices，俗称AMD，是群众眼中伟大的翻身大师，不屈的勇者，Intel斗士。</p>
<p>​<img src="/images/console/AMD.jpg" alt="AMD"></p>
<p>本来主机市场和它没有任何关系，一直在CPU市场和Intel拼刺刀，但是直到2006年这个神奇的时间点，与另外一个同样被打压的生活不能自理的ATi碰上了头，AMD收购了ATi，这才掀开了全新的篇章。</p>
<p>2006年Intel Core系列CPU发布，正式从前面的低谷中走出，而AMD在Intel的新攻势下节节败退，唯有DIY市场上凭借着自身产品的性价比保有了一隅之地。高端市场全面失守让AMD的利润大幅下降，在这时它看到了同样悲惨的ATi，于是两家在经过了协商后，宣布合并。</p>
<p>一个做CPU的和一个做GPU的合并，那么是不是可以把CPU与GPU拼在一起，让用户买一块芯片就能同时拥有两块的功能？这个思路的产物就是——Accelerated Processing Unit（APU），本质上来说，CPU还是CPU，GPU还是GPU，两者还是泾渭分明的关系，只是通过技术手段把两块芯片封装在了同一个容器里，让用户买一块APU就可以直接获得CPU+GPU的功能。</p>
<p>​                                                   </p>
<p><img src="/images/console/APU.jpg" alt="APU"></p>
<p>这对于降低用户的使用成本有着立竿见影的效果，但是虽然价格便宜了，对性能没有任何加成。这两家之所以被压的抬不起头还是因为产品的绝对性能和Intel与Nvidia拉开了差距，不得不采取这样的产品策略。</p>
<p>在微软和Sony各自经历了痛苦的初期之后，这时又杀出来一个黑马——Wii。</p>
<p>2006年11月19日，也就是Sony发布PlayStation 3的8天后，任天堂公布了最新一代主机Wii，不同于它们上一台主机Game Cube主打的高性能路线，这一代主机基本放弃了和另外两家比拼机能的想法，主打娱乐轻松的家庭共乐路线。</p>
<p>Wii的首发售价为249美金，同期的Xbox 360首发售价为299美金，而PS3则为惊人的499美金。核心玩家普遍不看好Wii，因为它是逆潮流的，性能孱弱，和3A游戏绝缘等因素。但是随后的销售情况让所有人都大吃一惊，Wii的销量一骑绝尘，任天堂的盈利节节攀升，甚至将任天堂送上了日本市值前十的地位。</p>
<p>于是乎在研发下一代主机的时候，微软与Sony不约而同的采取了同样的策略，那就是压缩成本。</p>
<p>2007 - 2010年的次贷危机让在全球范围内掀起了巨大的浪潮，人们迫于生计，纷纷削减了非必要的开支，失业人数也节节攀高。本来就处于落后的AMD日子也越发难过，笔记本市场可以说是退出了竞争，而显卡业务也处于半死不活的状态。就在这时，AMD的转机出现了。</p>
<p>由于Sony和微软都经历过亏本卖主机的时期，导致了它们不得不放弃以往的做法，转而追求硬件上的盈利（或者说少亏点）。本世代已经过半，双方都在着手筹备研发下一代主机，微软和Sony都学习到了足够多的经验，也有了足够的认识来避免踩更多的坑。虽然双方对于下一代主机的发展思路有分歧，比如微软准备继续发扬Kinect体感技术的优势，强化体感操作所占的比重，而Sony则彻底贯彻1080P才是一切的思路，砍掉所有多余的功能与噱头，专注于游戏本身。</p>
<p>微软在进行新一代主机设计的时候，思路非常明确，那就是绝对不能再出现三红这样的故障。这一思路的结果就是Xbox One初期型号那犹如PC主机大小的外形，而且拆开来看，Xbox One也和PC十分相似。宽大的机体保证了足够的空间来放置必要的散热系统，并且足够大内部空间不会让热量快速堆积在某个小地方，从根本上避免了三红出现的可能。</p>
<p>​                           </p>
<p><img src="/images/console/Xbox-One-360-stack.png" alt="Xbox-One-360-stack"></p>
<p>Sony也对自己的设计思路做了大刀阔斧的改进，Cell处理芯片果断抛弃，RSX图形处理芯片也一并放弃，将原本隔离的两个芯片通过特殊设计的机制结合起来，让CPU与GPU更好的互联互通，降低了第三方的游戏开发难度与成本。不仅如此，Sony对新一代主机所需要搭载的功能做了最大程度的简化，并且将操作系统所占用的内存最大可能的压缩，让游戏程序可以占用尽可能多的资源。</p>
<p>而微软和Sony能实现上述改进的基础，就是AMD的拳头产品——APU。</p>
<p>APU所搭载的CPU与GPU的就性能来讲，与同时期的竞争对手的产品相比，都处于中端产品的水平，但是二者合在一起所体现出来的价格优势，那就比分开采购大多了。加上AMD在此时已经潜心研发APU多年，对CPU与GPU的协同工作有了很多心得。AMD甚至可以让APU内置的GPU与另外一块独立的显卡进行联动，可见其技术积累在这方面十分深厚。加上游戏主机的芯片订单数量都是千万级别起，微软和Sony能谈下来的出厂价格肯定会进一步压低，这对双方来说都是十分利好的消息。</p>
<p>一方面Sony被金融危机所累，平板电视与其他家电业务也一直在下滑，PlayStation 3后期虽然销量不错，但前期巨大的投入与亏损让游戏业务的报表十分难看，所以降低下一代主机的成本成了当务之急。微软的Xbox业务也同样不妙，三红事件的影响实在太过深远，让整个公司内部对Xbox业务的存在意义产生了质疑，而且抢占家庭娱乐中心这一Xbox业务的战略目标达成情况不容乐观，虽然在北美市场Xbox 360销量非常不错，但是只有游戏这一个“功能”还过得去，微软想要通过Xbox来成为整个家庭的互联网中心的计划并不顺利。</p>
<p>AMD则亟需一个稳定持续的利润点来支撑越来越捉襟见肘的研发投入，因为产品的性能始终无法赶上竞争对手，导致AMD只能在中低端市场进行搏杀，而中低端市场的利润率又无法让AMD加码投入到研发当中，这样就陷入了一个恶性循环，而微软和Sony送上的大订单就是AMD翻身的关键（到2017年终于见到了回报）。一代主机少说6到7年，多则10年的销售周期让AMD在一定时间内可以安心进行新技术与产品的开发，而主机的销量也可以让AMD压低芯片的出厂价格，以薄利多销的形式来为这两家供货。</p>
<h2 id="6"><a href="#6" class="headerlink" title="6"></a>6</h2><p>那么我们的Nvidia此时在干什么呢？</p>
<p>Nvidia在为Sony定制了PlayStation 3的RSX芯片之后，公司内部觉得主机业务研发流程繁琐，因为都是深度定制，要根据主机厂商的软硬件组合进行开发，而且更重要的是利润率不高，一台主机的售价里能分给元器件的部分本来就不多，而Nvidia自己显卡零售业务那可都是实打实进到了自己口袋。而且在ATi与AMD合并之后，Nvidia在中高端市场处于垄断地位，高利润率让Nvidia对又麻烦，钱又少的主机业务失去了兴趣。于是乎次世代的主机里面没有Nvidia的身影，而AMD独占了这一块。</p>
<p>但这并不代表着Nvidia没有自己的野心，那就是CPU。</p>
<p>老黄（黄仁勋，Nvidia创始人兼CEO）是一个非常有野心的人，他不甘心自己的GPU一直得听Intel&#x2F;AMD的CPU指挥，于是他决定自己做CPU。当然，Nvidia那时候的体量当然不足以和Intel相抗衡，于是他没有直接杀入x86桌面CPU市场，而是转而将目光投向了欣欣向荣的移动市场，在ARM架构的基础上，融入了自己独步天下的GPU技术，打造出了自己的移动SoC产品线——Tegra，可能有细心的读者已经认出来了，这就是任天堂最新主机Nintendo Switch上搭载的芯片组。</p>
<p>​				</p>
<p><img src="/images/console/Tegra.jpg" alt="Tegra"></p>
<p>高通的Snapdragon也好，Nvidia的Tegra也好，以及苹果的A系列也好，相对于Intel以及AMD的芯片产品本质上没有多少区别(无非是指令集与处理效率的不同），不过因为所面向的市场不同，所以x86桌面端与ARM移动端并没有多少直接的竞争关系。Nvidia的Tegra的产品主打特色为GeForce加成的图形处理能力，当年搭载Tegra芯片的手机可以运行一些Nvidia特别移植过来的PC游戏，在那个抽卡手游还没横扫市场的年代还是令人十分惊艳的。而且Nvidia还基于Tegra推出了自己的掌机产品线——Nvidia Shield。不仅可以运行高画质的手机游戏，还能和自家是GeForce显卡进行联动，将电脑上的3A大作通过无线视频传输到掌机上进行游戏，基本就是PlayStation 4和PSV联动的最初版本。</p>
<p>​				</p>
<p><img src="/images/console/Shield.jpg" alt="Shield"></p>
<p>就像前面提到的，强大的性能带来了相对来说更多的热量，而Tegra将这领先时代的图形性能与发热量带到了体积更为狭小，散热条件更恶劣的手机&#x2F;平板电脑上。那一时期搭载Tegra的产品无一例外不是电老虎与暖手宝的合体，虽然有更强的游戏性能，但是掉电速度和发热量已经到了令人无法忍受的地步，Tegra在移动市场也只是昙花一现，后期就只有Nvidia Shield系列还在继续使用Tegra芯片组了。</p>
<p>虽然性能优越，但是功耗与发热量巨大，而且还必须单独为其进行优化与适配（是不是想到了Sony的某一款产品？），市场上变得无人问津，不过Nvidia并没有放弃Tegra，而是将它的定位从移动市场变换到了另一个移动市场，力图将其打造成为汽车娱乐系统的神经中枢。一直到2017年任天堂发售的Nintendo Switch才将Nvidia带回了主机市场这一阔别已久的领域。</p>
<p>相信大家对NS上的画面表现都印象深刻，Tegra性能优势一如既往，我在看过马里奥赛车8的表现之后确实佩服Nvidia在图形技术领域的积累，分屏之后还能保持帧率稳定，实在是令人惊叹。Nvidia在Tegra的尝试失败之后并没有止步不前，凭借着AI以及机器学习的大热，以及比特币等数字货币的兴起，显卡业务越来越好，而且Nvidia一直在布局未来，Nvidia对自动驾驶领域的研究与投入也越来越大，并且依托自身在GPU领域的技术实力，推出了一系列的自动驾驶解决方案，极有可能在未来抢得先机。</p>
<h2 id="7"><a href="#7" class="headerlink" title="7"></a>7</h2><p>终于要到结尾了，这篇文章写的十分随意，只有一个很粗略的时间线索，我也只是凭着我的记忆以及维基百科里面的内容拼凑出上面这么多文字，如果能看到这儿我想说一声辛苦了。</p>
<p>商业公司之间的行为也是会互相影响的，尤其是游戏界上下游关系如此明确的情况。上面这几家公司互相合作与翻脸，加上外界因素的作用，让各自的发展轨迹都有了很大的不同。</p>
<p>如果微软继续和Nvidia合作，那么三红事件有没有可能不会发生？而不发生三红事件的话，Xbox 360应该很有可能让微软彻底压制Sony吧。</p>
<p>而如果不是金融危机，那么AMD能不能拿到Sony和微软的主机订单，扛到2017年咸鱼翻身也是一个未知数。</p>
<p>DirectX的加入让主机游戏开发者有机会接触到PC游戏的开发流程，而微软与Sony的主机硬件基础实质上的统一而让跨平台与开发流程缩短了很多。但是由于3A游戏的过于强势让中小厂商无力竞争，导致市场分化越来越严重，游戏的画面不断进步让游戏开发流程又变得复杂而昂贵。</p>
<p>主机PC化已经是事实，两者的分界线已经只存于软件层面，而AMD的翻身会不会让Intel也加入到主机战场中，Nvidia还会不会与微软或者Sony重拾合作关系，一切都只能交给时间来回答。</p>
]]></content>
      <categories>
        <category>hardware</category>
      </categories>
      <tags>
        <tag>console</tag>
        <tag>amd</tag>
        <tag>nvidia</tag>
        <tag>microsoft</tag>
        <tag>gpu</tag>
        <tag>cpu</tag>
        <tag>sony</tag>
        <tag>nintendo</tag>
      </tags>
  </entry>
</search>
